{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import uniform\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.sparse import diags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "class GMM:\n",
    "    \"\"\"\n",
    "    Full covariance Gaussian Mixture Model,\n",
    "    trained using Expectation Maximization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int\n",
    "        Number of clusters/mixture components in which the data will be\n",
    "        partitioned into.\n",
    "\n",
    "    n_iters : int\n",
    "        Maximum number of iterations to run the algorithm.\n",
    "\n",
    "    tol : float\n",
    "        Tolerance. If the log-likelihood between two iterations is smaller than\n",
    "        the specified tolerance level, the algorithm will stop performing the\n",
    "        EM optimization.\n",
    "\n",
    "    seed : int\n",
    "        Seed / random state used to initialize the parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components: int, n_iters= 300, tol = 0.001, seed=0):\n",
    "        self.n_components = n_components\n",
    "        self.n_iters = n_iters\n",
    "        self.tol = tol\n",
    "        self.seed = seed\n",
    "\n",
    "    def init_parameter (self, X):\n",
    "        kmeans = KMeans(n_clusters=self.n_components, n_init=1, random_state=self.seed).fit(X)\n",
    "        labels = kmeans.labels_\n",
    "\n",
    "        n_row, n_col = X.shape\n",
    "        self.resp = np.zeros((n_row, self.n_components))\n",
    "        self.means = np.zeros((self.n_components, n_col))\n",
    "        self.covs = np.zeros((self.n_components, n_col, n_col))\n",
    "        self.weights = np.ones(self.n_components)\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            mask = np.where(labels == k)\n",
    "            self.means[k, :] = np.mean(X[mask], axis = 0)\n",
    "            x = X[mask] - self.means[k, :]\n",
    "            self.covs[k, :, :] = np.dot(self.weights[k] * x.T, x) / X[mask].shape[0]      \n",
    "\n",
    "    def init_parameters_rand (self, X):\n",
    "        # data's dimensionality and responsibility vector\n",
    "        n_row, n_col = X.shape     \n",
    "        self.resp = np.zeros((n_row, self.n_components))\n",
    "\n",
    "        # initialize parameters\n",
    "        np.random.seed(self.seed)\n",
    "        chosen = np.random.choice(n_row, self.n_components, replace = False)\n",
    "        self.means = X[chosen]\n",
    "        self.weights = np.full(self.n_components, 1 / self.n_components)\n",
    "        \n",
    "        # for np.cov, rowvar = False, \n",
    "        # indicates that the rows represents obervation\n",
    "        shape = self.n_components, n_col, n_col\n",
    "        self.covs = np.full(shape, np.cov(X, rowvar = False))\n",
    "\n",
    "    def fit(self, X):\n",
    "\n",
    "        self.init_parameters_rand(X)\n",
    "\n",
    "        log_likelihood = 0\n",
    "        self.converged = False\n",
    "        self.log_likelihood_trace = []      \n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            log_likelihood_new = self._do_estep(X)\n",
    "            self._do_mstep(X)\n",
    "\n",
    "            if abs(log_likelihood_new - log_likelihood) <= self.tol:\n",
    "                self.converged = True\n",
    "                print(i)\n",
    "                break\n",
    "  \n",
    "            log_likelihood = log_likelihood_new\n",
    "            self.log_likelihood_trace.append(log_likelihood)\n",
    "        \n",
    "        if (not self.converged):\n",
    "            print(self.n_iters)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _do_estep(self, X):\n",
    "        \"\"\"\n",
    "        E-step: compute responsibilities,\n",
    "        update resp matrix so that resp[j, k] is the responsibility of cluster k for data point j,\n",
    "        to compute likelihood of seeing data point j given cluster k, use multivariate_normal.pdf\n",
    "        \"\"\"\n",
    "        self._compute_log_likelihood(X)\n",
    "        log_likelihood = np.sum(np.log(np.sum(self.resp, axis = 1)))\n",
    "\n",
    "        # normalize over all possible cluster assignments\n",
    "        self.resp = self.resp / self.resp.sum(axis = 1, keepdims = 1)\n",
    "        return log_likelihood\n",
    "\n",
    "    def _compute_log_likelihood(self, X):\n",
    "        for k in range(self.n_components):\n",
    "            prior = self.weights[k]\n",
    "            likelihood = multivariate_normal(self.means[k], self.covs[k], allow_singular=True).pdf(X)\n",
    "            self.resp[:, k] = prior * likelihood\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _do_mstep(self, X):\n",
    "        \"\"\"M-step, update parameters\"\"\"\n",
    "\n",
    "        # total responsibility assigned to each cluster, N^{soft}\n",
    "        resp_weights = self.resp.sum(axis = 0)\n",
    "        \n",
    "        # weights\n",
    "        self.weights = resp_weights / X.shape[0]\n",
    "\n",
    "        # means\n",
    "        weighted_sum = np.dot(self.resp.T, X)\n",
    "        self.means = weighted_sum / resp_weights.reshape(-1, 1)\n",
    "        # covariance\n",
    "        for k in range(self.n_components):\n",
    "            diff = (X - self.means[k]).T\n",
    "            weighted_sum = np.dot(self.resp[:, k] * diff, diff.T)\n",
    "            self.covs[k] = weighted_sum / resp_weights[k]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n, dim = X.shape\n",
    "        test_data = np.asarray(X)\n",
    "        test_num_points = n\n",
    "        test_z = np.zeros((test_num_points, self.n_components))\n",
    "\n",
    "        for k in range(self.n_components):\n",
    "        \n",
    "            prior = self.weights[k]\n",
    "            likelihood = multivariate_normal(self.means[k], self.covs[k], allow_singular=True).pdf(X)\n",
    "            test_z[:, k] = prior * likelihood\n",
    "\n",
    "        test_z =  test_z /  test_z.sum(axis = 1, keepdims = 1)\n",
    "        \n",
    "        output = []\n",
    "        for row in test_z:\n",
    "            output.append(np.argmax(row))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('urbanGB.all/urbanGB.txt', header=None, names=['x', 'y'])\n",
    "df_label = pd.read_csv('urbanGB.all/urbanGB.labels.txt', header=None, names=['label'])\n",
    "df = df.join(df_label)\n",
    "df = df.sample(150000, random_state=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "standard_df = scaler.fit_transform(df.drop(['label'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import sklearn.metrics\n",
    "gm = GaussianMixture(n_components=5, random_state=0)\n",
    "gm.fit(standard_df)\n",
    "gm.n_iter_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklrn_output = gm.predict(standard_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49660397996577677"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.silhouette_score(standard_df, sklrn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GMM at 0x1bea15f5d80>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm = GMM(n_components=5)\n",
    "gmm.fit(standard_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_output = gmm.predict(standard_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2774864844608824"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.silhouette_score(standard_df, my_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a2ce51783487cb738dcf95c986f48432f9d5d3f1a7ed222d040a760409c430c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
